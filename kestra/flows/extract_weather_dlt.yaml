
id: extract_weather_dlt
namespace: aemetelt
description: |
  Best to add a label `backfill:true` from the UI to track executions created via a backfill.
  
tasks:
  - id: dlt_extract_weather_stations
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:      
      type: io.kestra.plugin.scripts.runner.docker.Docker    
    containerImage: python:3.13
    beforeCommands:
      - pip install "dlt[gs]"
      - pip install pyarrow
    env:
      WEATHER_STATIONS_TO_GCS__CREDENTIALS: "{{ kv('GCP_CREDS') }}"
      WEATHER_STATIONS_TO_GCS__DESTINATION__BUCKET_URL: "{{ kv('GCP_BUCKET_NAME') }}"
    warningOnStdErr: false
    script: |
      import dlt
      import http.client
      import json
      import requests

      api_key = "{{kv('AEMET_API_KEY')}}"

      # Configure the pipeline to write to GCS
      pipeline = dlt.pipeline(
          pipeline_name="weather_stations_to_gcs",
          destination="filesystem",
          dataset_name="weather_stations"
      )

      # Function to extract data from the API
      def extract_data():
          conn = http.client.HTTPSConnection("opendata.aemet.es")
          headers = {
              'cache-control': "no-cache"
          }
          conn.request("GET", f"/opendata/api/valores/climatologicos/inventarioestaciones/todasestaciones?api_key={api_key}", headers=headers)
          res = conn.getresponse()
          data = res.read()
          response_json = json.loads(data.decode("utf-8"))
          
          # Get the URL from the 'datos' field
          datos_url = response_json.get("datos")
          
          # Make a second request to the obtained URL
          response = requests.get(datos_url)
          response.raise_for_status()
          data = response.json()
          
          for record in data:
              yield record

      # Use dlt to load data from the obtained URL
      resource = dlt.resource(extract_data, name="weather_stations")

      # Run the pipeline to load data into GCS
      load_info = pipeline.run(resource, loader_file_format="parquet", write_disposition="replace")

      row_counts = pipeline.last_trace.last_normalize_info
      print(row_counts)
      print("------")
      print(load_info)
  
  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: To avoid cluttering your storage, we will remove the downloaded files

triggers:
  - id: green_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 9 1 * *"
    inputs:
      taxi: green
